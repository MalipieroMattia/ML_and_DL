{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras import layers, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebookâ€™s output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "# Input layer\n",
    "# Lower layer with ReLu activation function\n",
    "# Upper layer with softmax activation function\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'), #lower layer\n",
    "  tf.keras.layers.Dense(10, activation='softmax') #upper layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1700/1718 - accuracy: 0.7034 - loss: 1.0097 - val_accuracy: 0.7320 - val_loss: 0.9479Epoch 2/5\n",
      "1700/1718 - accuracy: 0.7917 - loss: 0.6258 - val_accuracy: 0.7972 - val_loss: 0.6108Epoch 3/5\n",
      "1700/1718 - accuracy: 0.8053 - loss: 0.5586 - val_accuracy: 0.8112 - val_loss: 0.5518Epoch 4/5\n",
      "1700/1718 - accuracy: 0.8184 - loss: 0.5227 - val_accuracy: 0.8186 - val_loss: 0.5214Epoch 5/5\n",
      "1700/1718 - accuracy: 0.8209 - loss: 0.5098 - val_accuracy: 0.8251 - val_loss: 0.5014"
     ]
    }
   ],
   "source": [
    "n_epochs = 5 \n",
    "batch_size = 32\n",
    "\n",
    "# Function to generate random batches\n",
    "def random_batch(X, y, batch_size):     \n",
    "    idx = np.random.randint(len(X), size=batch_size)     \n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# Function to print the training status\n",
    "def print_status_bar(step, total, train_loss, train_accuracy, val_loss, val_accuracy):\n",
    "    metrics_str = f\"accuracy: {train_accuracy.result():.4f} - loss: {train_loss.result():.4f} - val_accuracy: {val_accuracy.result():.4f} - val_loss: {val_loss.result():.4f}\"\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics_str, end=end)\n",
    "\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "# Define the optimizers with specified learning rates\n",
    "first_opt = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "second_opt = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy() #using sparse categorical cross entropy as a loss function\n",
    "train_loss = tf.keras.metrics.Mean(name=\"loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
    "\n",
    "# Custom Training Loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "\n",
    "        train_accuracy.update_state(y_batch, y_pred)\n",
    "        gradients=tape.gradient(loss,model.trainable_variables)\n",
    "\n",
    "        first_opt.apply_gradients(zip(gradients[:2],model.layers[1].trainable_variables)) #lower layer\n",
    "        second_opt.apply_gradients(zip(gradients[2:],model.layers[2].trainable_variables)) #upper layer\n",
    "        train_loss(loss)\n",
    "\n",
    "        # Print summary statistics and update them until the epoch end\n",
    "        if step % 100 == 0:\n",
    "            y_val_pred = model(X_valid, training=False)\n",
    "            val_loss.update_state(loss_fn(y_valid, y_val_pred))\n",
    "            val_accuracy.update_state(y_valid, y_val_pred)\n",
    "            print_status_bar(step, n_steps, train_loss, train_accuracy, val_loss, val_accuracy) #recalling the print_status_bar function\n",
    "\n",
    "    for metric in [train_loss, train_accuracy, val_loss, val_accuracy]:\n",
    "        metric.reset_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
