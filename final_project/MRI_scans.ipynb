{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c99b6fdc45b64d4994e387cb792dd2a3",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# ML Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File extraction from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data'):\n",
    "    shutil.rmtree('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following zip files to extract: ['1512427.zip', 'archive.zip', 'Brain Tumor data.zip']\n",
      "Successfully extracted '1512427.zip' to 'data\\1512427'\n",
      "Successfully extracted 'archive.zip' to 'data\\archive'\n",
      "Successfully extracted 'Brain Tumor data.zip' to 'data\\Brain Tumor data'\n",
      "Extraction process complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the folder name or path\n",
    "folder_name = \"data\"\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(folder_name, exist_ok=True)  \n",
    "\n",
    "# Path to zipped files\n",
    "base_zip_path = 'data_zipped' \n",
    "\n",
    "# Path for extracted files\n",
    "target_base_path = 'data'   \n",
    "\n",
    "# Create the target base path \n",
    "os.makedirs(target_base_path, exist_ok=True)\n",
    "\n",
    "# Check if the base_zip_path exists\n",
    "if not os.path.isdir(base_zip_path):\n",
    "    print(f\"Error: The directory '{base_zip_path}' was not found.\")\n",
    "else:\n",
    "    # List all files in the base_zip_path\n",
    "    all_files = os.listdir(base_zip_path)\n",
    "    \n",
    "    # Filter for .zip files\n",
    "    zip_files_to_extract = [f for f in all_files if f.lower().endswith('.zip')]\n",
    "\n",
    "    if not zip_files_to_extract:\n",
    "        print(f\"No .zip files found in '{base_zip_path}'.\")\n",
    "    else:\n",
    "        print(f\"Found the following zip files to extract: {zip_files_to_extract}\")\n",
    "\n",
    "        for zip_filename in zip_files_to_extract:\n",
    "            zip_file_path = os.path.join(base_zip_path, zip_filename)\n",
    "            \n",
    "            # Create a new folder name from the zip file name (without .zip)\n",
    "            folder_name = os.path.splitext(zip_filename)[0]\n",
    "            output_folder_path = os.path.join(target_base_path, folder_name)\n",
    "            \n",
    "            # Create the specific output folder \n",
    "            os.makedirs(output_folder_path, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(output_folder_path)\n",
    "                    print(f\"Successfully extracted '{zip_filename}' to '{output_folder_path}'\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting '{zip_filename}': {e}\")\n",
    "\n",
    "        print(\"Extraction process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PMRAM is divided in two folders, one being augmented data and one being raw data, we will drop the augmented data one and augment the raw data ourselves in the data augmentation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I had to adjust the path and add a second archive because it created a duplicated version when unzipping in the \n",
    "#previous passage. check again (SOLVED). \n",
    "#however we might have some problems if somnath automatically unzip files, but I don't think it would be the case\n",
    "shutil.rmtree('data/archive/PMRAM Bangladeshi Brain Cancer - MRI Dataset/PMRAM Bangladeshi Brain Cancer - MRI Dataset/Augmented Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change folder structure to make it easier to work with\n",
    "\n",
    "# Path to folder with actual data\n",
    "deep_folder = 'data/archive/PMRAM Bangladeshi Brain Cancer - MRI Dataset/PMRAM Bangladeshi Brain Cancer - MRI Dataset/Raw Data/Raw'\n",
    "\n",
    "# Desired new location\n",
    "new_base = 'data/PMRAM_Dataset/Raw Data'\n",
    "\n",
    "# Create the new destination\n",
    "os.makedirs(new_base, exist_ok=True)\n",
    "\n",
    "# Move contents\n",
    "for item in os.listdir(deep_folder):\n",
    "    src_path = os.path.join(deep_folder, item)\n",
    "    dst_path = os.path.join(new_base, item)\n",
    "    shutil.move(src_path, dst_path)\n",
    "\n",
    "# Delete the old empty folder tree\n",
    "shutil.rmtree('data/archive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Brain Tumor data is already in jpg format, and divided in training and testing splits and classes, we will change the split ourselves later. \n",
    "For now we remove the duplicated folder and change its name to improve readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database 1512427 contains various sub-folders made up of 4 .zip files with each .zip file containing 766 slices as can be read in the original README, we will unzip them and move them in a single folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following zip files to extract: ['brainTumorDataPublic_1-766.zip', 'brainTumorDataPublic_1533-2298.zip', 'brainTumorDataPublic_2299-3064.zip', 'brainTumorDataPublic_767-1532.zip']\n",
      "Successfully extracted 'brainTumorDataPublic_1-766.zip' to 'data/1512427\\brainTumorDataPublic_1-766'\n",
      "Successfully deleted 'brainTumorDataPublic_1-766.zip'\n",
      "Successfully extracted 'brainTumorDataPublic_1533-2298.zip' to 'data/1512427\\brainTumorDataPublic_1533-2298'\n",
      "Successfully deleted 'brainTumorDataPublic_1533-2298.zip'\n",
      "Successfully extracted 'brainTumorDataPublic_2299-3064.zip' to 'data/1512427\\brainTumorDataPublic_2299-3064'\n",
      "Successfully deleted 'brainTumorDataPublic_2299-3064.zip'\n",
      "Successfully extracted 'brainTumorDataPublic_767-1532.zip' to 'data/1512427\\brainTumorDataPublic_767-1532'\n",
      "Successfully deleted 'brainTumorDataPublic_767-1532.zip'\n",
      "\n",
      "Unzipping and deletion process complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the source directory containing the zip files\n",
    "source_zip_dir = 'data/1512427'\n",
    "\n",
    "# Define the target base directory for unzipped folders \n",
    "target_extract_base_dir = 'data/1512427'\n",
    "\n",
    "# Ensure the source directory exists\n",
    "if not os.path.isdir(source_zip_dir):\n",
    "    print(f\"Error: Source directory '{source_zip_dir}' not found.\")\n",
    "else:\n",
    "    zip_files_found = [f for f in os.listdir(source_zip_dir) if f.lower().endswith('.zip')]\n",
    "\n",
    "    if not zip_files_found:\n",
    "        print(f\"No .zip files found in '{source_zip_dir}'.\")\n",
    "    else:\n",
    "        print(f\"Found the following zip files to extract: {zip_files_found}\")\n",
    "\n",
    "        for zip_filename in zip_files_found:\n",
    "            zip_file_full_path = os.path.join(source_zip_dir, zip_filename)\n",
    "            \n",
    "            # Create a folder name from the zip file name (without .zip)\n",
    "            extraction_folder_name = os.path.splitext(zip_filename)[0]\n",
    "            output_folder_path = os.path.join(target_extract_base_dir, extraction_folder_name)\n",
    "            \n",
    "            # Create the specific output folder \n",
    "            os.makedirs(output_folder_path, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_file_full_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(output_folder_path)\n",
    "                    print(f\"Successfully extracted '{zip_filename}' to '{output_folder_path}'\")\n",
    "                \n",
    "                # Delete the zip file after successful extraction\n",
    "                os.remove(zip_file_full_path)\n",
    "                print(f\"Successfully deleted '{zip_filename}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during processing of '{zip_filename}': {e}\")\n",
    "                \n",
    "        print(\"\\nUnzipping and deletion process complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ca511e7e38e74cbba115bc47e6ab8b8d",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### converting MAT files into JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "8f921400177e4d8487e3102c874feb48",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "base_folder = 'data/1512427'\n",
    "\n",
    "\n",
    "input_folders = [\n",
    "    'brainTumorDataPublic_1-766',\n",
    "    'brainTumorDataPublic_767-1532',\n",
    "    'brainTumorDataPublic_1533-2298',\n",
    "    'brainTumorDataPublic_2299-3064'\n",
    "]\n",
    "\n",
    "\n",
    "# Output base folder\n",
    "output_folder = ('data/China_Dataset')\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Label to class name mapping\n",
    "label_map = {\n",
    "    1: 'meningioma',\n",
    "    2: 'glioma',\n",
    "    3: 'pituitary'\n",
    "}\n",
    "\n",
    "# Loop through input folders\n",
    "for folder_name in input_folders:\n",
    "    input_folder_path = os.path.join(base_folder, folder_name)\n",
    "    mat_files = [f for f in os.listdir(input_folder_path) if f.endswith('.mat')]\n",
    "\n",
    "    for mat_file in mat_files:\n",
    "        mat_file_path = os.path.join(input_folder_path, mat_file)\n",
    "\n",
    "        # Load .mat file\n",
    "        with h5py.File(mat_file_path, 'r') as f:\n",
    "            image = np.array(f['cjdata']['image']).T\n",
    "            label = int(np.array(f['cjdata']['label'])[0][0])\n",
    "\n",
    "        # Normalize image\n",
    "        image = image.astype(np.float64)\n",
    "        image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
    "\n",
    "        # Get class name from label\n",
    "        class_name = label_map.get(label, 'unknown')\n",
    "        class_folder = os.path.join(output_folder, class_name)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        # Save image\n",
    "        base_name = os.path.splitext(mat_file)[0]\n",
    "        output_path = os.path.join(class_folder, base_name + '.jpg')\n",
    "        Image.fromarray(image).save(output_path)\n",
    "\n",
    "# remove old folder containing MAT files\n",
    "shutil.rmtree('data/1512427')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now all the datasets are in jpg format. \n",
    "We will now proceed to do augmentation where needed and then we will move them all into a single folder to then start with model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definig the augmentation procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "def zoom_image(image, zoom_factor):\n",
    "    h, w = image.shape[:2]\n",
    "    new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
    "\n",
    "    resized = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "    if zoom_factor > 1:\n",
    "        # Crop center\n",
    "        start_x = (new_w - w) // 2\n",
    "        start_y = (new_h - h) // 2\n",
    "        return resized[start_y:start_y + h, start_x:start_x + w]\n",
    "    else:\n",
    "        # Pad\n",
    "        pad_x = (w - new_w) // 2\n",
    "        pad_y = (h - new_h) // 2\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            resized, pad_y, h - new_h - pad_y, pad_x, w - new_w - pad_x,\n",
    "            borderType=cv2.BORDER_REFLECT\n",
    "        )\n",
    "        return padded\n",
    "\n",
    "def apply_light_blur(image, kernel_size, sigma):\n",
    "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n",
    "\n",
    "def process_with_specific_augmentations(img_path, output_dir, prefix):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_original.jpg\"), img)\n",
    "    \n",
    "    rotated_cw = rotate_image(img, -10)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_rot_cw.jpg\"), rotated_cw)\n",
    "\n",
    "    rotated_ccw = rotate_image(img, 10)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_rot_ccw.jpg\"), rotated_ccw)\n",
    "\n",
    "    zoom_in = zoom_image(img, 1.1)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_zoom_in.jpg\"), zoom_in)\n",
    "\n",
    "    zoom_out = zoom_image(img, 0.9)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_zoom_out.jpg\"), zoom_out)\n",
    "\n",
    "    blurred = apply_light_blur(img, 3, 0.05)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{prefix}_blurred.jpg\"), blurred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "0427f6b902d04f75b79c865e8a6143a7",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resizing and augmentation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "# Base input and output directories\n",
    "input_base_dir = \"data\"\n",
    "if os.path.exists('processed_data'):\n",
    "    shutil.rmtree('processed_data')\n",
    "output_base_dir = \"processed_data\"\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "'''\n",
    "# Image augmentation config: Rotation & Zoom\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "'''\n",
    "\n",
    "# Traverse datasets\n",
    "for dataset_name in os.listdir(input_base_dir):\n",
    "    dataset_path = os.path.join(input_base_dir, dataset_name)\n",
    "\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                full_path = os.path.join(root, file)\n",
    "\n",
    "                # Create output path preserving relative structure\n",
    "                relative_path = os.path.relpath(root, input_base_dir)\n",
    "                out_folder = os.path.join(output_base_dir, relative_path)\n",
    "                os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "                prefix = Path(file).stem\n",
    "                process_with_specific_augmentations(full_path, out_folder, prefix)\n",
    "\n",
    "print(\"Image resizing and augmentation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "78deed334d6e417fb9b7b8caf1a84b45",
  "deepnote_persisted_session": {
   "createdAt": "2025-05-06T12:31:23.781Z"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
